{
 "metadata": {
  "name": "",
  "signature": "sha256:2acdc41e3ebb71f0b2995265027ff026654af058b422320a67aa237e6a5138cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Inputs**\n",
      "\n",
      "- Feature List = Original (FULL)\n",
      "- Folds = 1000\n",
      "- K (SelectKBest) = 'all'\n",
      "- Grid Search = StratifiedShuffleSplit\n",
      "\n",
      "**Algorithm**\n",
      "\n",
      "Decision Tree Classifer\n",
      "\n",
      "**Outputs**\n",
      "\n",
      "- Feature Importances Frequency\n",
      "- Average Feature Importances Score\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Use the outputs to select feature for final analysis through intuition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Iteration 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Full original features (except email address)\n",
      "\n",
      "features_list =  ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus',\n",
      "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses',\n",
      "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees',\n",
      "                  'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
      "                  'shared_receipt_with_poi']\n",
      "\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "#np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "    \n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n",
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n",
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 22 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'salary',\n",
        " 'deferral_payments',\n",
        " 'total_payments',\n",
        " 'loan_advances',\n",
        " 'bonus',\n",
        " 'restricted_stock_deferred',\n",
        " 'deferred_income',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'long_term_incentive',\n",
        " 'restricted_stock',\n",
        " 'director_fees',\n",
        " 'to_messages',\n",
        " 'from_poi_to_this_person',\n",
        " 'from_messages',\n",
        " 'from_this_person_to_poi',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_from_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 88.699 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.089,\n",
        " 'deferral_payments': 0.007,\n",
        " 'deferred_income': 0.035,\n",
        " 'director_fees': 0.0,\n",
        " 'exercised_stock_options': 0.144,\n",
        " 'expenses': 0.107,\n",
        " 'fraction_from_poi': 0.008,\n",
        " 'fraction_to_poi': 0.147,\n",
        " 'from_messages': 0.017,\n",
        " 'from_poi_to_this_person': 0.011,\n",
        " 'from_this_person_to_poi': 0.019,\n",
        " 'loan_advances': 0.001,\n",
        " 'long_term_incentive': 0.03,\n",
        " 'other': 0.119,\n",
        " 'restricted_stock': 0.037,\n",
        " 'restricted_stock_deferred': 0.001,\n",
        " 'salary': 0.019,\n",
        " 'shared_receipt_with_poi': 0.11,\n",
        " 'to_messages': 0.009,\n",
        " 'total_payments': 0.032,\n",
        " 'total_stock_value': 0.058}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 392,\n",
        " 'deferral_payments': 58,\n",
        " 'deferred_income': 242,\n",
        " 'exercised_stock_options': 760,\n",
        " 'expenses': 712,\n",
        " 'fraction_from_poi': 71,\n",
        " 'fraction_to_poi': 903,\n",
        " 'from_messages': 155,\n",
        " 'from_poi_to_this_person': 89,\n",
        " 'from_this_person_to_poi': 205,\n",
        " 'loan_advances': 12,\n",
        " 'long_term_incentive': 248,\n",
        " 'other': 770,\n",
        " 'restricted_stock': 272,\n",
        " 'restricted_stock_deferred': 10,\n",
        " 'salary': 140,\n",
        " 'shared_receipt_with_poi': 666,\n",
        " 'to_messages': 87,\n",
        " 'total_payments': 264,\n",
        " 'total_stock_value': 560}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.80933\tPrecision: 0.28521\tRecall: 0.28550\tF1: 0.28536\tF2: 0.28544\n",
        "\tTotal predictions: 15000\tTrue positives:  571\tFalse positives: 1431\tFalse negatives: 1429\tTrue negatives: 11569\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Iteration 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Full original features (except email address)\n",
      "\n",
      "features_list =  ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus',\n",
      "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses',\n",
      "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees',\n",
      "                  'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
      "                  'shared_receipt_with_poi']\n",
      "\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "#np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "    \n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n",
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n",
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 22 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'salary',\n",
        " 'deferral_payments',\n",
        " 'total_payments',\n",
        " 'loan_advances',\n",
        " 'bonus',\n",
        " 'restricted_stock_deferred',\n",
        " 'deferred_income',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'long_term_incentive',\n",
        " 'restricted_stock',\n",
        " 'director_fees',\n",
        " 'to_messages',\n",
        " 'from_poi_to_this_person',\n",
        " 'from_messages',\n",
        " 'from_this_person_to_poi',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_from_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 86.943 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.086,\n",
        " 'deferral_payments': 0.008,\n",
        " 'deferred_income': 0.035,\n",
        " 'director_fees': 0.0,\n",
        " 'exercised_stock_options': 0.145,\n",
        " 'expenses': 0.108,\n",
        " 'fraction_from_poi': 0.01,\n",
        " 'fraction_to_poi': 0.148,\n",
        " 'from_messages': 0.018,\n",
        " 'from_poi_to_this_person': 0.011,\n",
        " 'from_this_person_to_poi': 0.02,\n",
        " 'loan_advances': 0.001,\n",
        " 'long_term_incentive': 0.029,\n",
        " 'other': 0.113,\n",
        " 'restricted_stock': 0.036,\n",
        " 'restricted_stock_deferred': 0.001,\n",
        " 'salary': 0.018,\n",
        " 'shared_receipt_with_poi': 0.113,\n",
        " 'to_messages': 0.009,\n",
        " 'total_payments': 0.034,\n",
        " 'total_stock_value': 0.057}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 397,\n",
        " 'deferral_payments': 69,\n",
        " 'deferred_income': 255,\n",
        " 'exercised_stock_options': 767,\n",
        " 'expenses': 731,\n",
        " 'fraction_from_poi': 85,\n",
        " 'fraction_to_poi': 893,\n",
        " 'from_messages': 159,\n",
        " 'from_poi_to_this_person': 87,\n",
        " 'from_this_person_to_poi': 207,\n",
        " 'loan_advances': 13,\n",
        " 'long_term_incentive': 227,\n",
        " 'other': 745,\n",
        " 'restricted_stock': 248,\n",
        " 'restricted_stock_deferred': 6,\n",
        " 'salary': 137,\n",
        " 'shared_receipt_with_poi': 669,\n",
        " 'to_messages': 86,\n",
        " 'total_payments': 281,\n",
        " 'total_stock_value': 557}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.80933\tPrecision: 0.28521\tRecall: 0.28550\tF1: 0.28536\tF2: 0.28544\n",
        "\tTotal predictions: 15000\tTrue positives:  571\tFalse positives: 1431\tFalse negatives: 1429\tTrue negatives: 11569\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Iteration 3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Full original features (except email address)\n",
      "\n",
      "features_list =  ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus',\n",
      "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses',\n",
      "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees',\n",
      "                  'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
      "                  'shared_receipt_with_poi']\n",
      "\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "#np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "    \n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n",
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n",
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 22 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'salary',\n",
        " 'deferral_payments',\n",
        " 'total_payments',\n",
        " 'loan_advances',\n",
        " 'bonus',\n",
        " 'restricted_stock_deferred',\n",
        " 'deferred_income',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'long_term_incentive',\n",
        " 'restricted_stock',\n",
        " 'director_fees',\n",
        " 'to_messages',\n",
        " 'from_poi_to_this_person',\n",
        " 'from_messages',\n",
        " 'from_this_person_to_poi',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_from_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 87.073 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.087,\n",
        " 'deferral_payments': 0.007,\n",
        " 'deferred_income': 0.035,\n",
        " 'director_fees': 0.0,\n",
        " 'exercised_stock_options': 0.14,\n",
        " 'expenses': 0.108,\n",
        " 'fraction_from_poi': 0.009,\n",
        " 'fraction_to_poi': 0.148,\n",
        " 'from_messages': 0.016,\n",
        " 'from_poi_to_this_person': 0.01,\n",
        " 'from_this_person_to_poi': 0.019,\n",
        " 'loan_advances': 0.001,\n",
        " 'long_term_incentive': 0.029,\n",
        " 'other': 0.121,\n",
        " 'restricted_stock': 0.038,\n",
        " 'restricted_stock_deferred': 0.001,\n",
        " 'salary': 0.018,\n",
        " 'shared_receipt_with_poi': 0.112,\n",
        " 'to_messages': 0.009,\n",
        " 'total_payments': 0.035,\n",
        " 'total_stock_value': 0.056}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 413,\n",
        " 'deferral_payments': 54,\n",
        " 'deferred_income': 242,\n",
        " 'exercised_stock_options': 766,\n",
        " 'expenses': 733,\n",
        " 'fraction_from_poi': 73,\n",
        " 'fraction_to_poi': 902,\n",
        " 'from_messages': 133,\n",
        " 'from_poi_to_this_person': 88,\n",
        " 'from_this_person_to_poi': 209,\n",
        " 'loan_advances': 23,\n",
        " 'long_term_incentive': 241,\n",
        " 'other': 772,\n",
        " 'restricted_stock': 285,\n",
        " 'restricted_stock_deferred': 8,\n",
        " 'salary': 154,\n",
        " 'shared_receipt_with_poi': 668,\n",
        " 'to_messages': 96,\n",
        " 'total_payments': 265,\n",
        " 'total_stock_value': 547}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.80933\tPrecision: 0.28521\tRecall: 0.28550\tF1: 0.28536\tF2: 0.28544\n",
        "\tTotal predictions: 15000\tTrue positives:  571\tFalse positives: 1431\tFalse negatives: 1429\tTrue negatives: 11569\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "print\n",
      "print 'Features originally chosen:'\n",
      "\n",
      "features_list = ['poi', 'bonus', 'total_stock_value', 'expenses',\n",
      "                 'exercised_stock_options', 'other','shared_receipt_with_poi'] # You will need to use more features\n",
      "\n",
      "print\n",
      "pp.pprint(features_list)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    #my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "fraction_features = ['fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "\n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Features originally chosen:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi']\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 8 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 82.516 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.106,\n",
        " 'exercised_stock_options': 0.154,\n",
        " 'expenses': 0.163,\n",
        " 'fraction_to_poi': 0.176,\n",
        " 'other': 0.185,\n",
        " 'shared_receipt_with_poi': 0.157,\n",
        " 'total_stock_value': 0.059}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 504,\n",
        " 'exercised_stock_options': 789,\n",
        " 'expenses': 872,\n",
        " 'fraction_to_poi': 985,\n",
        " 'other': 938,\n",
        " 'shared_receipt_with_poi': 851,\n",
        " 'total_stock_value': 485}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.84786\tPrecision: 0.46460\tRecall: 0.42650\tF1: 0.44473\tF2: 0.43361\n",
        "\tTotal predictions: 14000\tTrue positives:  853\tFalse positives:  983\tFalse negatives: 1147\tTrue negatives: 11017\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "print\n",
      "print 'Features originally chosen:'\n",
      "\n",
      "features_list = ['poi', 'bonus', 'total_stock_value', 'expenses',\n",
      "                 'exercised_stock_options', 'other','shared_receipt_with_poi'] # You will need to use more features\n",
      "\n",
      "print\n",
      "pp.pprint(features_list)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    #my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "fraction_features = ['fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "\n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Features originally chosen:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi']\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 8 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 101.988 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.106,\n",
        " 'exercised_stock_options': 0.149,\n",
        " 'expenses': 0.158,\n",
        " 'fraction_to_poi': 0.178,\n",
        " 'other': 0.186,\n",
        " 'shared_receipt_with_poi': 0.161,\n",
        " 'total_stock_value': 0.06}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 505,\n",
        " 'exercised_stock_options': 787,\n",
        " 'expenses': 864,\n",
        " 'fraction_to_poi': 989,\n",
        " 'other': 942,\n",
        " 'shared_receipt_with_poi': 870,\n",
        " 'total_stock_value': 498}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.84786\tPrecision: 0.46460\tRecall: 0.42650\tF1: 0.44473\tF2: 0.43361\n",
        "\tTotal predictions: 14000\tTrue positives:  853\tFalse positives:  983\tFalse negatives: 1147\tTrue negatives: 11017\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "print\n",
      "print 'Features originally chosen:'\n",
      "\n",
      "features_list = ['poi', 'bonus', 'total_stock_value', 'expenses',\n",
      "                 'exercised_stock_options', 'other','shared_receipt_with_poi'] # You will need to use more features\n",
      "\n",
      "print\n",
      "pp.pprint(features_list)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    #my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "fraction_features = ['fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "\n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(len(important_names_score)):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Features originally chosen:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi']\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 8 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 92.108 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.103,\n",
        " 'exercised_stock_options': 0.146,\n",
        " 'expenses': 0.164,\n",
        " 'fraction_to_poi': 0.182,\n",
        " 'other': 0.188,\n",
        " 'shared_receipt_with_poi': 0.156,\n",
        " 'total_stock_value': 0.06}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 476,\n",
        " 'exercised_stock_options': 763,\n",
        " 'expenses': 882,\n",
        " 'fraction_to_poi': 996,\n",
        " 'other': 942,\n",
        " 'shared_receipt_with_poi': 851,\n",
        " 'total_stock_value': 486}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.84786\tPrecision: 0.46460\tRecall: 0.42650\tF1: 0.44473\tF2: 0.43361\n",
        "\tTotal predictions: 14000\tTrue positives:  853\tFalse positives:  983\tFalse negatives: 1147\tTrue negatives: 11017\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "print\n",
      "print 'Features originally chosen:'\n",
      "\n",
      "features_list = ['poi', 'bonus', 'total_stock_value', 'expenses',\n",
      "                 'exercised_stock_options', 'other','shared_receipt_with_poi'] # You will need to use more features\n",
      "\n",
      "print\n",
      "pp.pprint(features_list)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    #my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "fraction_features = ['fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k=4\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "\n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(4):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Features originally chosen:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi']\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 8 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= 4\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 84.255 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.249,\n",
        " 'exercised_stock_options': 0.291,\n",
        " 'expenses': 0.298,\n",
        " 'total_stock_value': 0.161}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 993,\n",
        " 'exercised_stock_options': 1000,\n",
        " 'expenses': 989,\n",
        " 'total_stock_value': 931}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.84786\tPrecision: 0.46460\tRecall: 0.42650\tF1: 0.44473\tF2: 0.43361\n",
        "\tTotal predictions: 14000\tTrue positives:  853\tFalse positives:  983\tFalse negatives: 1147\tTrue negatives: 11017\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "### for measuring time taken\n",
      "\n",
      "import time\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "print\n",
      "print 'Features originally chosen:'\n",
      "\n",
      "features_list = ['poi', 'bonus', 'expenses', 'exercised_stock_options',\n",
      "                 'other','shared_receipt_with_poi'] # You will need to use more features\n",
      "\n",
      "print\n",
      "pp.pprint(features_list)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
      "\n",
      "### Task 2: Remove outliers\n",
      "\n",
      "print\n",
      "print 'Outliers removed:'\n",
      "print\n",
      "print '[GRAMM WENDY L]'\n",
      "print\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print '[LOCKHART EUGENE E]'\n",
      "print\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print '[WROBEL BRUCE]'\n",
      "print\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print '[THE TRAVEL AGENCY IN THE PARK]'\n",
      "print\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print '[TOTAL]'\n",
      "data_dict.pop(\"TOTAL\", None)\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print\n",
      "print 'Inconsistent records updated:'\n",
      "print\n",
      "print '[BELFER ROBERT]'\n",
      "print\n",
      "print '[BHATNAGAR SANJAY]'\n",
      "\n",
      "\n",
      "data_dict['BELFER ROBERT'] = {'bonus': 'NaN',\n",
      "                              'deferral_payments': 'NaN',\n",
      "                              'deferred_income': -102500,\n",
      "                              'director_fees': 102500,\n",
      "                              'email_address': 'NaN',\n",
      "                              'exercised_stock_options': 'NaN',\n",
      "                              'expenses': 3285,\n",
      "                              'from_messages': 'NaN',\n",
      "                              'from_poi_to_this_person': 'NaN',\n",
      "                              'from_this_person_to_poi': 'NaN',\n",
      "                              'loan_advances': 'NaN',\n",
      "                              'long_term_incentive': 'NaN',\n",
      "                              'other': 'NaN',\n",
      "                              'poi': False,\n",
      "                              'restricted_stock': -44093,\n",
      "                              'restricted_stock_deferred': 44093,\n",
      "                              'salary': 'NaN',\n",
      "                              'shared_receipt_with_poi': 'NaN',\n",
      "                              'to_messages': 'NaN',\n",
      "                              'total_payments': 3285,\n",
      "                              'total_stock_value': 'NaN'}\n",
      "\n",
      "data_dict['BHATNAGAR SANJAY'] = {'bonus': 'NaN',\n",
      "                                 'deferral_payments': 'NaN',\n",
      "                                 'deferred_income': 'NaN',\n",
      "                                 'director_fees': 'NaN',\n",
      "                                 'email_address': 'sanjay.bhatnagar@enron.com',\n",
      "                                 'exercised_stock_options': 15456290,\n",
      "                                 'expenses': 137864,\n",
      "                                 'from_messages': 29,\n",
      "                                 'from_poi_to_this_person': 0,\n",
      "                                 'from_this_person_to_poi': 1,\n",
      "                                 'loan_advances': 'NaN',\n",
      "                                 'long_term_incentive': 'NaN',\n",
      "                                 'other': 'NaN',\n",
      "                                 'poi': False,\n",
      "                                 'restricted_stock': 2604490,\n",
      "                                 'restricted_stock_deferred': -2604490,\n",
      "                                 'salary': 'NaN',\n",
      "                                 'shared_receipt_with_poi': 463,\n",
      "                                 'to_messages': 523,\n",
      "                                 'total_payments': 137864,\n",
      "                                 'total_stock_value': 15456290} \n",
      "\n",
      "### Task 3: Create new feature(s)\n",
      "\n",
      "print\n",
      "print 'Creating 2 new features to be used as part of the final analysis'\n",
      "\n",
      "\n",
      "### Store to my_dataset for easy export below.\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "      \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "   \n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    #my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "print   \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'\n",
      "\n",
      "\n",
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "fraction_features = ['fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "np_my_feature_list = np.array(my_feature_list)\n",
      "\n",
      "np_my_feature_list = np_my_feature_list[1::]\n",
      "\n",
      "print\n",
      "print 'Number of features now:', len(my_feature_list), \"and the final features list:\"\n",
      "print\n",
      "pp.pprint(my_feature_list)\n",
      "\n",
      "\n",
      "### Extract features and labels from dataset for local testing\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "### Task 4: Try a varity of classifiers\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithm\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "\n",
      "\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "folds = 1000\n",
      "cv = StratifiedShuffleSplit(labels, folds)\n",
      "\n",
      "\n",
      "### feature selection, because text is super high dimensional and \n",
      "### can be really computationally chewy as a result\n",
      "\n",
      "k='all'\n",
      "selector = SelectKBest(f_classif, k)\n",
      "\n",
      "\n",
      "### counter for number of iterations\n",
      "\n",
      "count = 0\n",
      "\n",
      "### dictionaries to document feature importances\n",
      "\n",
      "feature_importances_frequency = {}\n",
      "feature_importances_score = {}\n",
      "\n",
      "### to measure duration of model training\n",
      "\n",
      "t0 = time.time()\n",
      "\n",
      "print\n",
      "print 'Feature Selection is performed using SelectKBest and k=', k\n",
      "time.sleep(5)\n",
      "print 'Cross Validation is performed using StratifiedShuffleSplit with', folds, 'folds'\n",
      "time.sleep(5)\n",
      "print 'Algorithm used is Decision Tree Classifier'\n",
      "time.sleep(5)\n",
      "print 'Feature Importances are documented, frequency + average scores are printed out for further analysis'\n",
      "time.sleep(5)\n",
      "print\n",
      "print 'Training the Model'\n",
      "print\n",
      "\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_train_fit_transformed = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    features_test_transformed = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    ### Decision Tree Classifier code chunk\n",
      "    \n",
      "    dt = DecisionTreeClassifier()\n",
      "    \n",
      "    parameters = {'random_state': [1, 50], 'max_features':('auto', 'sqrt', 'log2'), 'criterion':('gini', 'entropy')}\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train_fit_transformed, labels_train)\n",
      "    pred = clf.predict(features_test_transformed)\n",
      "    \n",
      "     ### Feature Importance documentation code chunk\n",
      "    \n",
      "    dt.fit(features_train_fit_transformed, labels_train)\n",
      "    importances = dt.feature_importances_\n",
      "    important_names_frequency = np_my_feature_list[importances > 0.05]\n",
      "    important_names_score = np_my_feature_list\n",
      "    \n",
      "    count += 1\n",
      "    if count%10 == 0:\n",
      "        print '*',\n",
      "    if count%100 == 0:\n",
      "        print count/10,'%'\n",
      "    \n",
      "\n",
      "    for v in range(len(important_names_frequency)):\n",
      "         \n",
      "        if important_names_frequency[v] in feature_importances_frequency:\n",
      "            \n",
      "            feature_importances_frequency[important_names_frequency[v]] += 1\n",
      "        else:\n",
      "            feature_importances_frequency[important_names_frequency[v]] = 1\n",
      "            \n",
      "    for v in range(4):\n",
      "         \n",
      "        if important_names_score[v] in feature_importances_score:\n",
      "\n",
      "            feature_importances_score[important_names_score[v]] += importances[v]\n",
      "        else:\n",
      "            feature_importances_score[important_names_score[v]] = 0.0\n",
      "                \n",
      "                \n",
      "for key, value in feature_importances_score.items():\n",
      "    feature_importances_score[key] = round((value / 1000), 3)       \n",
      "\n",
      "print \n",
      "print 'Model Trained'\n",
      "print    \n",
      "print \"Total time taken to train:\", round(time.time()-t0, 3), \"s\"    \n",
      "print\n",
      "print 'Average Score of Feature Importances'\n",
      "print\n",
      "pp.pprint(feature_importances_score) \n",
      "print\n",
      "print 'Frequency of Feature Importances'\n",
      "print    \n",
      "pp.pprint(feature_importances_frequency) \n",
      "print\n",
      "\n",
      "\n",
      "\n",
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script.\n",
      "### Because of the small size of the dataset, the script uses stratified\n",
      "### shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
      "\n",
      "print\n",
      "print 'Passing the result to tester.py to evaluate Performance Metrics'\n",
      "\n",
      "test_classifier(clf, my_dataset, my_feature_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Features originally chosen:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi']\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Outliers removed:\n",
        "\n",
        "[GRAMM WENDY L]\n",
        "\n",
        "[LOCKHART EUGENE E]\n",
        "\n",
        "[WROBEL BRUCE]\n",
        "\n",
        "[THE TRAVEL AGENCY IN THE PARK]\n",
        "\n",
        "[TOTAL]\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inconsistent records updated:\n",
        "\n",
        "[BELFER ROBERT]\n",
        "\n",
        "[BHATNAGAR SANJAY]\n",
        "\n",
        "Creating 2 new features to be used as part of the final analysis\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n",
        "\n",
        "Number of features now: 7 and the final features list:\n",
        "\n",
        "['poi',\n",
        " 'bonus',\n",
        " 'expenses',\n",
        " 'exercised_stock_options',\n",
        " 'other',\n",
        " 'shared_receipt_with_poi',\n",
        " 'fraction_to_poi']\n",
        "\n",
        "Feature Selection is performed using SelectKBest and k= all\n",
        "Cross Validation is performed using StratifiedShuffleSplit with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 folds\n",
        "Algorithm used is Decision Tree Classifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importances are documented, frequency + average scores are printed out for further analysis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the Model\n",
        "\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 10 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 20 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 30 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 40 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 50 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 60 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 70 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 80 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 90 %\n",
        "*"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " * "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "* 100 %\n",
        "\n",
        "Model Trained\n",
        "\n",
        "Total time taken to train: 85.478 s\n",
        "\n",
        "Average Score of Feature Importances\n",
        "\n",
        "{'bonus': 0.115,\n",
        " 'exercised_stock_options': 0.187,\n",
        " 'expenses': 0.165,\n",
        " 'other': 0.194}\n",
        "\n",
        "Frequency of Feature Importances\n",
        "\n",
        "{'bonus': 554,\n",
        " 'exercised_stock_options': 950,\n",
        " 'expenses': 889,\n",
        " 'fraction_to_poi': 994,\n",
        " 'other': 944,\n",
        " 'shared_receipt_with_poi': 857}\n",
        "\n",
        "\n",
        "Passing the result to tester.py to evaluate Performance Metrics\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'random_state': [1, 50], 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tAccuracy: 0.83079\tPrecision: 0.40160\tRecall: 0.37650\tF1: 0.38865\tF2: 0.38127\n",
        "\tTotal predictions: 14000\tTrue positives:  753\tFalse positives: 1122\tFalse negatives: 1247\tTrue negatives: 10878\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}