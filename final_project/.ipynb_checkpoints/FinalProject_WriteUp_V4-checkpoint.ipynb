{
 "metadata": {
  "name": "",
  "signature": "sha256:246c369d77e1b9b228fdedf78c1f9ddd786be2ca0e83fa9da9ea349a37de6544"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introduction to Machine Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "by Saad Khan 05/25/2015"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications, and pulp and paper companies, with claimed revenues of nearly $111 billion during 2000. Fortune named Enron \"America's Most Innovative Company\" for six consecutive years.\n",
      "\n",
      "The Enron scandal, revealed in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.\n",
      "\n",
      "In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE:** ENRON submission free response question are answered as part of this project report"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  As part of your answer, give some background on the dataset and how it can be used to answer the project question.  Were there any outliers in the data when you got it, and how did you handle those?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Project Goal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of the project is to build a person of Interest (POI) identifier, using  various machine learning techniques, based on the email messages and financial data made public as a result fo the Enron scandal. POIs in this case are poeple who worked at Enron and were probable suspects for the fraudulent activites performed there which eventually led to its bankruptcy. Machine learning will help predict if a certain person picked at random who worked at Enron was involved in the corporate fraud or not. Concepts of machine learning are to be applied to the available dataset and a POI classifier to be created which would help predict person of interest."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import statements and loading the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "### basic python import statements\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pprint as pp\n",
      "\n",
      "### to make ipython notebook inline matplotlib graphics for ease of viewing\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### path and import statements to access folder functions\n",
      "\n",
      "\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the important characteristics of the dataset are as follows:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dimensions of the Data Set (Total number of points)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Total number of observations/data points in the data set:', len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of observations/data points in the data set: 146\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Total number of features available per observation:', sum(len(v) for v in data_dict.itervalues())/len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of features available per observation: 21\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Allocation of obervations (POIs vs. Non-POIs)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "for k, v in data_dict.iteritems():\n",
      "    if(data_dict[k][\"poi\"]==1):\n",
      "        count +=1\n",
      "    else:\n",
      "        continue\n",
      "print 'Number of obervations that have POIs:', count\n",
      "print\n",
      "print 'Number of obervations that do not have POIs:', len(data_dict) - count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of obervations that have POIs: 18\n",
        "\n",
        "Number of obervations that do not have POIs: 128\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Salary and Total payments of some high ups @ Enron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'LAY KENNETH L, Salary:', data_dict['LAY KENNETH L']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']\n",
      "print\n",
      "print 'SKILLING JEFFREY K, Salary:', data_dict['SKILLING JEFFREY K']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']\n",
      "print\n",
      "print 'FASTOW ANDREW S: Salary,', data_dict['FASTOW ANDREW S']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LAY KENNETH L, Salary: 1072321 , Total payments: 103559793\n",
        "\n",
        "SKILLING JEFFREY K, Salary: 1111258 , Total payments: 103559793\n",
        "\n",
        "FASTOW ANDREW S: Salary, 440698 , Total payments: 103559793\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Salaried people @ Enron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "salaried_people = 0\n",
      "for k,v in data_dict.iteritems():\n",
      "    if (data_dict[k][\"salary\"]!= 'NaN'):\n",
      "        salaried_people += 1\n",
      "    else:\n",
      "        continue\n",
      "print 'Number of people salaried @ Enron:', salaried_people"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of people salaried @ Enron: 95\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Outlier Investigation (Detection and Removal / Handling)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outliers in the data set can result from some malfunction or data entry errors. I analyzed the financial data document 'enron61702insiderpay.pdf' with naked eye without any code and following are the observations that I think are outliers and should be cleaned away as they do not contain any essential information. I handled the outliers using the piece of code I applied in the lesson 5 excercise in the format: 'dictionary.pop( key, 0 )'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 2: Remove outliers\n",
      "print 'Wendy Grahm only has the feature of directors fees'\n",
      "print\n",
      "### pp.pprint(data_dict['GRAMM WENDY L'])\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print 'Eugene Lockhart does not have value associated with any feature'\n",
      "print\n",
      "### pp.pprint(data_dict['LOCKHART EUGENE E'])\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print 'Bruce Wrobel only has exercised stock options as feature'\n",
      "print\n",
      "### pp.pprint(data_dict['WROBEL BRUCE'])\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print 'THE TRAVEL AGENCY IN THE PARK does not seem to be a POI'\n",
      "print\n",
      "### pp.pprint(data_dict['THE TRAVEL AGENCY IN THE PARK'])\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print 'TOTAL is not a particular POI as its the total of all financial features'\n",
      "print\n",
      "### pp.pprint(data_dict['TOTAL'])\n",
      "data_dict.pop(\"TOTAL\", None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wendy Grahm only has the feature of directors fees\n",
        "\n",
        "Eugene Lockhart does not have value associated with any feature\n",
        "\n",
        "Bruce Wrobel only has exercised stock options as feature\n",
        "\n",
        "THE TRAVEL AGENCY IN THE PARK does not seem to be a POI\n",
        "\n",
        "TOTAL is not a particular POI as its the total of all financial features\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimized Feature Selection/Engineering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section deals with the importantance of features in the dataset. The 3 important aspects covered here are as follows:\n",
      "\n",
      "- Creation of new features\n",
      "- Intelligent selection of features\n",
      "- Proper scaling of features (if required)\n",
      "\n",
      "List of all 21 original features in the dataset are as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pp.pprint(data_dict['LAY KENNETH L'].keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['salary',\n",
        " 'to_messages',\n",
        " 'deferral_payments',\n",
        " 'total_payments',\n",
        " 'exercised_stock_options',\n",
        " 'bonus',\n",
        " 'restricted_stock',\n",
        " 'shared_receipt_with_poi',\n",
        " 'restricted_stock_deferred',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'loan_advances',\n",
        " 'from_messages',\n",
        " 'other',\n",
        " 'from_this_person_to_poi',\n",
        " 'poi',\n",
        " 'director_fees',\n",
        " 'deferred_income',\n",
        " 'long_term_incentive',\n",
        " 'email_address',\n",
        " 'from_poi_to_this_person']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial Feature Selection (by hand from the original features in the datset)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "### POI label\n",
      "\n",
      "poi = 'poi'\n",
      "\n",
      "### NOTE: features combinations are setup in such a way so that performance can be determined individually\n",
      "\n",
      "### Email features\n",
      "\n",
      "e_feature_1 = 'from_messages'\n",
      "e_feature_2 = 'to_messages'\n",
      "e_feature_3 = 'shared_receipt_with_poi'\n",
      "\n",
      "# e_features_list = [e_feature_1, e_feature_2, e_feature_3]\n",
      "\n",
      "e_features_list = []\n",
      "e_features_list = [e_feature_1]\n",
      "e_features_list = e_features_list + [e_feature_2]\n",
      "e_features_list = e_features_list + [e_feature_3]\n",
      "\n",
      "\n",
      "### Financial features\n",
      "\n",
      "f_feature_1 = 'salary'\n",
      "f_feature_2 = 'bonus'\n",
      "f_feature_3 = 'exercised_stock_options'\n",
      "f_feature_4 = 'total_stock_value'\n",
      "f_feature_5 = 'loan_advances'\n",
      "\n",
      "# f_features_list = [f_feature_1, f_feature_2, f_feature_3, f_feature_4, f_feature_5]\n",
      "\n",
      "f_features_list = []\n",
      "f_features_list = [f_feature_1]\n",
      "f_features_list = f_features_list + [f_feature_2]\n",
      "f_features_list = f_features_list + [f_feature_3]\n",
      "f_features_list = f_features_list + [f_feature_4]\n",
      "f_features_list = f_features_list + [f_feature_5]\n",
      "\n",
      "\n",
      "# fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "### Combined features\n",
      "\n",
      "#features_list = [poi] + e_features_list + f_features_list\n",
      "\n",
      "features_list =  ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus',\n",
      "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses',\n",
      "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees',\n",
      "                  'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi',\n",
      "                  'shared_receipt_with_poi']\n",
      "\n",
      "print 'Number of features used from the original data set:', len(features_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features used from the original data set: 20\n"
       ]
      }
     ],
     "prompt_number": 355
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Invesitgation/Relationship of some of the features that were selected by hand"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this purpose, few of the features in the dataset were plotted to have a general idea about he distribution. Data points marked wih red crosses are POIs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = featureFormat(data_dict, features_list )\n",
      "poi, testing_poi_test_features = targetFeatureSplit(data)\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][1], testing_poi_test_features[ii][2], color = 'c')\n",
      "    plt.xlabel('to_messages')\n",
      "    plt.ylabel('shared_receipt_with_poi')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][1], testing_poi_test_features[ii][2], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][0], testing_poi_test_features[ii][2], color = 'c')\n",
      "    plt.xlabel('from_messages')\n",
      "    plt.ylabel('shared_receipt_with_poi')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][0], testing_poi_test_features[ii][2], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][3], testing_poi_test_features[ii][5], color = 'c')\n",
      "    plt.xlabel('salary')\n",
      "    plt.ylabel('bonus')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][3], testing_poi_test_features[ii][5], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][6], testing_poi_test_features[ii][5], color = 'c')\n",
      "    plt.xlabel('exercised_stock_options')\n",
      "    plt.ylabel('bonus')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][6], testing_poi_test_features[ii][5], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEQCAYAAAD2/KAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZGZmhXACMjOzQvx/nyEVeI5kt+sAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x16791588>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEQCAYAAAD2/KAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x167915f8>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEVCAYAAADHKRPdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x17ceab38>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6V4rpQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1720c7b8>"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "New Feature Creation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Fraction to/from POI emails (2 features) - Combination of original features in the dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the exercise quiz in lesson 11, following code was implemented to include 2 new features in the data set namely 'fraction_from_poi' and 'fraction_to_poi'. These features are a combination of 4 original features in the dataset. After creation, features are then stored in the modified feature list, 'my_feature_list' and values for all observations stored in modified data set, 'my_dataset'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Assignment to my_dataset and my_feature_list\n",
      "\n",
      "my_dataset = data_dict\n",
      "my_feature_list = features_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 356
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 3: Create new feature(s)\n",
      "#my_feature_list = []\n",
      "#my_dataset = {}\n",
      "\n",
      "### computeFraction function used from lesson 11 exercise\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "    \"\"\" given a number messages to/from POI (numerator) \n",
      "        and number of all messages to/from a person (denominator),\n",
      "        return the fraction of messages to/from that person\n",
      "        that are from/to a POI\n",
      "   \"\"\"\n",
      "\n",
      "\n",
      "    ### you fill in this code, so that it returns either\n",
      "    ###     the fraction of all messages to this person that come from POIs\n",
      "    ###     or\n",
      "    ###     the fraction of all messages from this person that are sent to POIs\n",
      "    ### the same code can be used to compute either quantity\n",
      "\n",
      "    ### beware of \"NaN\" when there is no known email address (and so\n",
      "    ### no filled email features), and integer division!\n",
      "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    \n",
      "    # print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "    # print fraction_from_poi\n",
      "    \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "    # print fraction_to_poi\n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi\n",
      "    \n",
      "print '2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 new features, \"fraction_from_poi\" and \"fraction_to_poi\", added to \"my_dataset\"\n"
       ]
      }
     ],
     "prompt_number": 357
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "print 'Number of features now:', len(my_feature_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features now: 22\n"
       ]
      }
     ],
     "prompt_number": 358
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poi  = \"poi\"\n",
      "\n",
      "poi_fraction_features = [poi, 'fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "data = featureFormat(my_dataset, poi_fraction_features)\n",
      "poi, testing_fraction_features = targetFeatureSplit(data)\n",
      "\n",
      "for ii, pp in enumerate(testing_fraction_features):\n",
      "    if testing_fraction_features[ii][0] != 0.0 or testing_fraction_features[ii][1] != 0.0:\n",
      "        plt.scatter(testing_fraction_features[ii][0], testing_fraction_features[ii][1], color = 'c')\n",
      "        plt.xlabel('Fraction of emails this person gets from POI')\n",
      "        plt.ylabel('Fraction of emails this person sends to POI')\n",
      "    else:\n",
      "        continue\n",
      "for ii, pp in enumerate(testing_fraction_features):\n",
      "    if poi[ii] and (testing_fraction_features[ii][0] != 0.0 or testing_fraction_features[ii][1] != 0.0):\n",
      "        plt.scatter(testing_fraction_features[ii][0], testing_fraction_features[ii][1], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3lS7ZIKdAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x160f6630>"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE:** The scatter plot represents the fraction of email exchange between Non-POI/POI. Red crosses show the POIs. It can be observed that there is a definite trend that POIs mostly send/receive emails to/from other POIs. People that are non POI tend to send fewer emails as can be seen mostly clustered with fractions closer to 0."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Word features from text in the email archives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pasreOutText function is used from the lesson 10 mini-project along with a modified version of the code used to extract email text in lesson 10 mini-project. Firstm it was checked if the observation had valid email address and then for a valid email address, email text was extracted using the pasreOutText function. The snowball stemmer for english is already implemented in the pasreOutText function. The text features data and the user labels data are extracted and stored in 'your_word_data.pkl' and 'your_email_authors.pkl' files respectively. Using words in emails as feature will add an additional dimension to he model and might be useful in creating the POI identifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseOutText(f):\n",
      "    \"\"\" given an opened email file f, parse out all text below the\n",
      "        metadata block at the top\n",
      "        (in Part 2, you will also add stemming capabilities)\n",
      "        and return a string that contains all the words\n",
      "        in the email (space-separated) \n",
      "        \n",
      "        example use case:\n",
      "        f = open(\"email_file_name.txt\", \"r\")\n",
      "        text = parseOutText(f)\n",
      "        \n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "    f.seek(0)  ### go back to beginning of file (annoying)\n",
      "    all_text = f.read()\n",
      "\n",
      "    ### split off metadata\n",
      "    content = all_text.split(\"X-FileName:\")\n",
      "    words = \"\"\n",
      "    if len(content) > 1:\n",
      "        ### remove punctuation\n",
      "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
      "\n",
      "        ### project part 2: comment out the line below\n",
      "        #words = text_string\n",
      "\n",
      "        ### split the text string into individual words, stem each word,\n",
      "        ### and append the stemmed word to words (make sure there's a single\n",
      "        ### space between each stemmed word)\n",
      "        \n",
      "        #print text_string\n",
      "        \n",
      "        email_split = text_string.split()\n",
      "    \n",
      "        #print email_split\n",
      "\n",
      "        from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "        stemmer = SnowballStemmer(\"english\")\n",
      "        \n",
      "        email_stemmed = []\n",
      "        for n in range(len(email_split)):\n",
      "            email_stemmed.append(stemmer.stem(email_split[n]))\n",
      "        \n",
      "    words = ' '.join(email_stemmed)    \n",
      "    \n",
      "    return words\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 209
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE:** Below is the piece of code which is used to identify valid email addresses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "valid_email_id_list = []\n",
      "\n",
      "email_address_path_folder = os.listdir(\"../final_project/emails_by_address/\")\n",
      "\n",
      "test_dataset = my_dataset\n",
      "#observation_removal = []\n",
      "\n",
      "count1 = 0\n",
      "count2 = 0\n",
      "\n",
      "for name in my_dataset:\n",
      "    #k = name\n",
      "    email_name = test_dataset[name]\n",
      "    email_address = email_name[\"email_address\"]\n",
      "    email_address_path = \"from_\" + email_address + \".txt\"\n",
      "    #print email_address, email_address_path\n",
      "    if email_address == 'NaN':\n",
      "        #observation_removal.append(name)\n",
      "        #print 'TO BE REMOVED_1:' ,email_address_path\n",
      "        count1 += 1\n",
      "    \n",
      "    elif email_address_path not in email_address_path_folder:\n",
      "        #observation_removal.append(name)\n",
      "        #print 'TO BE REMOVED_2:' ,email_address_path\n",
      "        count2 += 1\n",
      "    else:\n",
      "        valid_email_id_list.append(email_address)\n",
      "        #print email_address_path\n",
      "    \n",
      "\n",
      "print 'size of the dataset:', len(my_dataset)    \n",
      "print    \n",
      "print 'No of observation that have no email data for word features:', count1 + count2 \n",
      "\n",
      "\n",
      "'''        \n",
      "for n in observation_removal:\n",
      "    test_dataset.pop(n, None)\n",
      "'''            \n",
      "\n",
      "my_dataset = test_dataset       \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of the dataset: 141\n",
        "\n",
        "No of observation that have no email data for word features: 55\n"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print valid_email_id_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['mark.metts@enron.com', 'bill.cordes@enron.com', 'kevin.hannon@enron.com', 'rockford.meyer@enron.com', 'jeffrey.mcmahon@enron.com', 'stanley.horton@enron.com', 'greg.piper@enron.com', 'gene.humphrey@enron.com', 'adam.umanoff@enron.com', 'jeremy.blachman@enron.com', 'marty.sunde@enron.com', 'dana.gibbs@enron.com', 'wes.colwell@enron.com', 's..muller@enron.com', 'charlene.jackson@enron.com', 'rob.walls@enron.com', 'louise.kitchen@enron.com', 'jeffrey.shankman@enron.com', 'rick.bergsieker@enron.com', 'philippe.bibi@enron.com', 'paula.rieker@enron.com', 'sally.beck@enron.com', 'david.haug@enron.com', 'gary.hickerson@enron.com', 'richard.lewis@enron.com', 'robert.hayes@enron.com', 'danny.mccarty@enron.com', 'dan.leff@enron.com', 'john.lavorato@enron.com', 'ken.powers@enron.com', 'james.bannantine@enron.com', 'richard.shapiro@enron.com', 'john.sherriff@enron.com', 'rex.shelby@enron.com', 'joseph.deffner@enron.com', 'greg.whalley@enron.com', 'mike.mcconnell@enron.com', 'jim.piro@enron.com', 'david.delainey@enron.com', 'kenneth.lay@enron.com', 'cindy.olson@enron.com', 'rebecca.mcdonald@enron.com', 'george.mcclellan@enron.com', 'mark.haedicke@enron.com', 'raymond.bowen@enron.com', 'jay.fitzgerald@enron.com', 'michael.moran@enron.com', 'brian.redmond@enron.com', 'tim.belden@enron.com', 'w.duran@enron.com', 'terence.thorn@enron.com', 'tracy.foy@enron.com', 'christopher.calger@enron.com', 'ken.rice@enron.com', 'vince.kaminski@enron.com', 'chip.cox@enron.com', 'jeff.skilling@enron.com', 'jeffrey.sherrick@enron.com', 'mark.pickering@enron.com', 'steven.kean@enron.com', 'kulvinder.fowler@enron.com', 'george.wasaff@enron.com', 'phillip.allen@enron.com', 'vicki.sharp@enron.com', 'michael.brown@enron.com', 'james.hughes@enron.com', 'sanjay.bhatnagar@enron.com', 'rebecca.carter@enron.com', 'john.buchanan@enron.com', 'julia.murray@enron.com', 'kevin.garland@enron.com', 'keith.dodson@enron.com', 'janet.dietrich@enron.com', 'james.derrick@enron.com', 'mark.frevert@enron.com', 'rod.hayslett@enron.com', 'jim.fallon@enron.com', 'mark.koenig@enron.com', 'larry.izzo@enron.com', 'elizabeth.tilney@enron.com', 'a..martin@enron.com', 'rick.buy@enron.com', 'richard.causey@enron.com', 'mitchell.taylor@enron.com', 'jeff.donahue@enron.com', 'ben.glisan@enron.com']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Final piece of code used to parse email text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from_data = []\n",
      "word_data = []\n",
      "\n",
      "import os\n",
      "import pickle\n",
      "import re\n",
      "import sys\n",
      "import string\n",
      "\n",
      "\n",
      "temp_counter = 0\n",
      "\n",
      "for n in range(len(valid_email_id_list)):\n",
      "    \n",
      "    email_address_folder = \"../final_project/emails_by_address/from_\" + valid_email_id_list[n] + \".txt\"\n",
      "    \n",
      "    #text_file = os.listdir(\"../final_project/emails_by_address/\")\n",
      "    \n",
      "    email_file_path = open(email_address_folder, \"r\")\n",
      "    \n",
      "    for path in email_file_path:\n",
      "        ### only look at first 200 emails when developing\n",
      "        ### once everything is working, remove this line to run over full dataset\n",
      "        temp_counter += 1\n",
      "        if temp_counter < 200:\n",
      "        \n",
      "        \n",
      "            path = os.path.join('..', path[:-1])\n",
      "            #print path\n",
      "            email = open(path, \"r\")\n",
      "\n",
      "            ### use parseOutText to extract the text from the opened email\n",
      "\n",
      "            ### use str.replace() to remove any instances of the words\n",
      "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
      "\n",
      "            ### append the text to word_data\n",
      "\n",
      "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
      "\n",
      "            stemmed_email = parseOutText(email)\n",
      "\n",
      "            word_data.append(stemmed_email)\n",
      "            '''\n",
      "            if (name == \"chris\"):\n",
      "                from_data.append(1)\n",
      "            else:\n",
      "                from_data.append(0)\n",
      "            '''\n",
      "            from_data.append(n)\n",
      "\n",
      "            email.close()\n",
      "    \n",
      "print \"emails processed\"\n",
      "#from_sara.close()\n",
      "#from_chris.close()\n",
      "\n",
      "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
      "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "emails processed\n"
       ]
      }
     ],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Extract features and labels from dataset for testing\n",
      "\n",
      "#data = featureFormat(my_dataset, my_feature_list)\n",
      "\n",
      "#labels, features = targetFeatureSplit(data)\n",
      "\n",
      "data = featureFormat(my_dataset, my_feature_list, sort_keys = True)\n",
      "\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "#print len(labels)\n",
      "\n",
      "#print len(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 352
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### python import statements for implementing the classifier\n",
      "\n",
      "### Vectorizer utility\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "### Feature Scaler utility\n",
      "\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "### Feature Selector utility\n",
      "\n",
      "from sklearn.feature_selection import SelectPercentile, f_classif\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "\n",
      "### Grid Fit/Transform utility\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "### Algorithms\n",
      "\n",
      "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "\n",
      "\n",
      "### Cross Validation utility\n",
      "\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "### Key Performance Indicators/Metrics\n",
      "\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 322
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE:** Going forward, I have implemented different combinations of features, vectorizer, scaler, feature selector and cross validation techniques to evaluate the Key Perfomance Indicators (KPIs) i.e. Precision and Recall along with accuracy and F1-score. I have provided the description for all parameters/features settings for the different combinations I tried to fullfil the requirement in the project rubric."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Combination 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Features selected :** Original features + fraction features + word features\n",
      "- **Algorithm         :** Ada Boost\n",
      "- **Cross-Validation  :** StratifiedKFold\n",
      "- **Vectorizer        :** Tfidf (stop_words=\"english\", lowercase=True)\n",
      "- **Scaler            :** MinMax\n",
      "- **Selector          :** Select Percentile (10%)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=3)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    print features_train\n",
      "    print\n",
      "    print word_features_train\n",
      "    print\n",
      "    print labels_train\n",
      "    print '+++++++++++++++++++++++++++++++++++++++++'    \n",
      "    print features_test\n",
      "    print\n",
      "    print word_features_test\n",
      "    print\n",
      "    print labels_test\n",
      "    print '----------------------------'   \n",
      "    '''    \n",
      "        \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    '''\n",
      "    print features_train\n",
      "    print\n",
      "    print features_test\n",
      "    '''\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 3\n",
      "print \"Average precision: \", sum(precision) / 3\n",
      "print \"Average recall: \", sum(recall) / 3\n",
      "print \"Average f1_score: \", sum(f1) / 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'word_data' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-145-f99d3f6a696c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfeatures_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mword_features_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mlabels_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'word_data' is not defined"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 100], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.83769\tPrecision: 0.39216\tRecall: 0.10000\tF1: 0.15936\tF2: 0.11751\n",
        "\tTotal predictions: 1300\tTrue positives:   20\tFalse positives:   31\tFalse negatives:  180\tTrue negatives: 1069\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 100], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.84308\tPrecision: 0.46552\tRecall: 0.13500\tF1: 0.20930\tF2: 0.15734\n",
        "\tTotal predictions: 1300\tTrue positives:   27\tFalse positives:   31\tFalse negatives:  173\tTrue negatives: 1069\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 100], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.82154\tPrecision: 0.25758\tRecall: 0.08500\tF1: 0.12782\tF2: 0.09815\n",
        "\tTotal predictions: 1300\tTrue positives:   17\tFalse positives:   49\tFalse negatives:  183\tTrue negatives: 1051\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.80231\tPrecision: 0.33136\tRecall: 0.28000\tF1: 0.30352\tF2: 0.28896\n",
        "\tTotal predictions: 1300\tTrue positives:   56\tFalse positives:  113\tFalse negatives:  144\tTrue negatives:  987\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "71"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "91"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "93"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "97"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.78462\tPrecision: 0.28723\tRecall: 0.27000\tF1: 0.27835\tF2: 0.27328\n",
        "\tTotal predictions: 1300\tTrue positives:   54\tFalse positives:  134\tFalse negatives:  146\tTrue negatives:  966\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=5)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.80846\tPrecision: 0.35152\tRecall: 0.29000\tF1: 0.31781\tF2: 0.30052\n",
        "\tTotal predictions: 1300\tTrue positives:   58\tFalse positives:  107\tFalse negatives:  142\tTrue negatives:  993\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=5)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 1000, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.80031\tPrecision: 0.29919\tRecall: 0.22200\tF1: 0.25488\tF2: 0.23408\n",
        "\tTotal predictions: 13000\tTrue positives:  444\tFalse positives: 1040\tFalse negatives: 1556\tTrue negatives: 9960\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=5)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.81385\tPrecision: 0.35417\tRecall: 0.25500\tF1: 0.29651\tF2: 0.27013\n",
        "\tTotal predictions: 1300\tTrue positives:   51\tFalse positives:   93\tFalse negatives:  149\tTrue negatives: 1007\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=5)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    '''\n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'kernel': ('linear', 'rbf'), 'C': [1, 10]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.82692\tPrecision: 0.28814\tRecall: 0.08500\tF1: 0.13127\tF2: 0.09895\n",
        "\tTotal predictions: 1300\tTrue positives:   17\tFalse positives:   42\tFalse negatives:  183\tTrue negatives: 1058\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=5)\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "#selector = SelectKBest(f_classif, k=3)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2')}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best'),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'max_features': ('auto', 'sqrt', 'log2'), 'criterion': ('gini', 'entropy')},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.82333\tPrecision: 0.31429\tRecall: 0.27500\tF1: 0.29333\tF2: 0.28205\n",
        "\tTotal predictions: 1500\tTrue positives:   55\tFalse positives:  120\tFalse negatives:  145\tTrue negatives: 1180\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, max_df = 0.5)\n",
      "scaler = MinMaxScaler()\n",
      "#selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=10)\n",
      "#cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 6)\n",
      "selector = SelectKBest(f_classif, k=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "#count = 0\n",
      "for train_indices, test_indices in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    #count += 1\n",
      "    #print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GaussianNB()\n",
        "\tAccuracy: 0.81560\tPrecision: 0.30000\tRecall: 0.33333\tF1: 0.31579\tF2: 0.32609\n",
        "\tTotal predictions:  141\tTrue positives:    6\tFalse positives:   14\tFalse negatives:   12\tTrue negatives:  109\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "#selector = SelectPercentile(f_classif, percentile=10)\n",
      "#skf = StratifiedKFold(labels, n_folds=10)\n",
      "#cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "cv = KFold( len(labels), 10)\n",
      "selector = SelectKBest(f_classif, k=6)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "8\n",
        "9\n",
        "10\n",
        "GaussianNB()\n",
        "\tAccuracy: 0.86525\tPrecision: 0.46667\tRecall: 0.38889\tF1: 0.42424\tF2: 0.40230\n",
        "\tTotal predictions:  141\tTrue positives:    7\tFalse positives:    8\tFalse negatives:   11\tTrue negatives:  115\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 266
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "#selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=6)\n",
      "#cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
      "#cv = KFold( len(labels), 10)\n",
      "selector = SelectKBest(f_classif, k=4)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 100], 'random_state': [1, 50], 'algorithm': ['SAMME.R']},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.85106\tPrecision: 0.38462\tRecall: 0.27778\tF1: 0.32258\tF2: 0.29412\n",
        "\tTotal predictions:  141\tTrue positives:    5\tFalse positives:    8\tFalse negatives:   13\tTrue negatives:  115\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 285
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=2)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm': ['SAMME.R']},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.87333\tPrecision: 0.60000\tRecall: 0.15000\tF1: 0.24000\tF2: 0.17647\n",
        "\tTotal predictions:  150\tTrue positives:    3\tFalse positives:    2\tFalse negatives:   17\tTrue negatives:  128\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 321
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=2)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.83333\tPrecision: 0.38095\tRecall: 0.40000\tF1: 0.39024\tF2: 0.39604\n",
        "\tTotal predictions:  150\tTrue positives:    8\tFalse positives:   13\tFalse negatives:   12\tTrue negatives:  117\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 327
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=2)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'criterion':['entropy']}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50], 'criterion': ['entropy']},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.80667\tPrecision: 0.28571\tRecall: 0.30000\tF1: 0.29268\tF2: 0.29703\n",
        "\tTotal predictions:  150\tTrue positives:    6\tFalse positives:   15\tFalse negatives:   14\tTrue negatives:  115\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 331
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=2)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.82000\tPrecision: 0.31579\tRecall: 0.30000\tF1: 0.30769\tF2: 0.30303\n",
        "\tTotal predictions:  150\tTrue positives:    6\tFalse positives:   13\tFalse negatives:   14\tTrue negatives:  117\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 332
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=3)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.83333\tPrecision: 0.33333\tRecall: 0.25000\tF1: 0.28571\tF2: 0.26316\n",
        "\tTotal predictions:  150\tTrue positives:    5\tFalse positives:   10\tFalse negatives:   15\tTrue negatives:  120\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 333
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=4)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.86000\tPrecision: 0.47059\tRecall: 0.40000\tF1: 0.43243\tF2: 0.41237\n",
        "\tTotal predictions:  150\tTrue positives:    8\tFalse positives:    9\tFalse negatives:   12\tTrue negatives:  121\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 334
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "skf = StratifiedKFold(labels, n_folds=6)\n",
      "#cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=4)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.84397\tPrecision: 0.30000\tRecall: 0.16667\tF1: 0.21429\tF2: 0.18293\n",
        "\tTotal predictions:  141\tTrue positives:    3\tFalse positives:    7\tFalse negatives:   15\tTrue negatives:  116\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 335
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=5)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.82667\tPrecision: 0.33333\tRecall: 0.30000\tF1: 0.31579\tF2: 0.30612\n",
        "\tTotal predictions:  150\tTrue positives:    6\tFalse positives:   12\tFalse negatives:   14\tTrue negatives:  118\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 336
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=6)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.84667\tPrecision: 0.28571\tRecall: 0.10000\tF1: 0.14815\tF2: 0.11494\n",
        "\tTotal predictions:  150\tTrue positives:    2\tFalse positives:    5\tFalse negatives:   18\tTrue negatives:  125\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 337
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=4)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n",
        "\tAccuracy: 0.86667\tPrecision: 0.50000\tRecall: 0.30000\tF1: 0.37500\tF2: 0.32609\n",
        "\tTotal predictions:  150\tTrue positives:    6\tFalse positives:    6\tFalse negatives:   14\tTrue negatives:  124\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 339
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PERF_FORMAT_STRING = \"\\\n",
      "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
      "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
      "\n",
      "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "\n",
      "#skf = StratifiedKFold(labels, n_folds=6)\n",
      "cv = StratifiedShuffleSplit(labels, 10, random_state = 42)\n",
      "#cv = KFold( len(labels), 6, shuffle=False, random_state=None)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=4)\n",
      "#selector = SelectPercentile(f_classif, percentile=5)\n",
      "\n",
      "true_negatives = 0\n",
      "false_negatives = 0\n",
      "true_positives = 0\n",
      "false_positives = 0\n",
      "count = 0\n",
      "for train_indices, test_indices in cv:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "    \n",
      "    \n",
      "    '''\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "    '''\n",
      "    \n",
      "    ### Partitioning of the data into training and testing datasets\n",
      "    \n",
      "    features_train = [features[ii] for ii in train_indices]\n",
      "    #word_features_train = [word_data[ii] for ii in train_indices]\n",
      "    labels_train = [labels[ii] for ii in train_indices]\n",
      "       \n",
      "    features_test = [features[jj] for jj in test_indices]\n",
      "    #word_features_test = [word_data[jj] for jj in test_indices]\n",
      "    labels_test = [labels[jj] for jj in test_indices]\n",
      "        \n",
      "    ### Fitting and transformation of the training dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    #features_train = np.hstack((features_train, word_features_train))\n",
      "    \n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    \n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    \n",
      "    ### Transformation of the test dataset (vectorizer, scaling, feature selection)\n",
      "    \n",
      "    #word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    #features_test = np.hstack((features_test, word_features_test))\n",
      "    \n",
      "    features_test = scaler.transform(features_test)\n",
      "    \n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ### Training the classifer\n",
      "    \n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50], 'algorithm' : ['SAMME.R']}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    parameters = {'criterion':('gini', 'entropy'), 'max_features':('auto', 'sqrt', 'log2'), 'random_state': [1, 50]}\n",
      "    dt = DecisionTreeClassifier()\n",
      "    clf = GridSearchCV(dt, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    print dt.feature_importances_\n",
      "    \n",
      "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "    svr = SVC()\n",
      "    clf = GridSearchCV(svr, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    clf = GaussianNB()\n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    rfc = RandomForestClassifier()\n",
      "    parameters = {'n_estimators': [1, 200], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(rfc, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ### Evaluating performance metrics\n",
      "    \n",
      "    #print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    #print \"Precision: \", precision_score(labels_test, pred)\n",
      "    #print \"Recall: \", recall_score(labels_test, pred)\n",
      "    #print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "    \n",
      "    for prediction, truth in zip(pred, labels_test):\n",
      "        if prediction == 0 and truth == 0:\n",
      "            true_negatives += 1\n",
      "        elif prediction == 0 and truth == 1:\n",
      "            false_negatives += 1\n",
      "        elif prediction == 1 and truth == 0:\n",
      "            false_positives += 1\n",
      "        else:\n",
      "            true_positives += 1\n",
      "    count += 1\n",
      "    print count\n",
      "try:\n",
      "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
      "\n",
      "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
      "\n",
      "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
      "\n",
      "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
      "\n",
      "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
      "\n",
      "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
      "\n",
      "    print clf\n",
      "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
      "    print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
      "    print \"\"\n",
      "except:\n",
      "    print \"Got a divide by zero when trying out:\", clf\n",
      "    '''\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "    '''\n",
      "#print \"Average accuracy: \", sum(accuracy) / 6\n",
      "#print \"Average precision: \", sum(precision) / 6\n",
      "#print \"Average recall: \", sum(recall) / 6\n",
      "#print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got a divide by zero when trying out: GridSearchCV(cv=None,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 200], 'random_state': [1, 50]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)\n"
       ]
      }
     ],
     "prompt_number": 359
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}