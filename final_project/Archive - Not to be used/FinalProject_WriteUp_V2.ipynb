{
 "metadata": {
  "name": "",
  "signature": "sha256:7043de759c026e4b9a083180e9b7555772fc66b17a9e9e6d05d6ed1ad55f79a8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction to Machine Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "by Saad Khan 05/25/2015"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications, and pulp and paper companies, with claimed revenues of nearly $111 billion during 2000. Fortune named Enron \"America's Most Innovative Company\" for six consecutive years.\n",
      "\n",
      "The Enron scandal, revealed in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.\n",
      "\n",
      "In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Project Goal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of the project is to build a person of Interest (POI) identifier based on the email messeges and financial data made public as a result fo the Enron scandal."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import statements and loading the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "\n",
      "\n",
      "import sys\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import string\n",
      "\n",
      "### to make ipython notebook inline matplotlib graphics for ease of viewing\n",
      "%matplotlib inline\n",
      "\n",
      "sys.path.append(\"../tools/\")\n",
      "\n",
      "import pprint as pp\n",
      "\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import test_classifier, dump_classifier_and_data\n",
      "\n",
      "### Load the dictionary containing the dataset\n",
      "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  As part of your answer, give some background on the dataset and how it can be used to answer the project question.  Were there any outliers in the data when you got it, and how did you handle those?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the important characteristics of the dataset are as follows:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dimensions of the Data Set (Total number of points)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Total number of observations/data points in the data set:', len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of observations/data points in the data set: 146\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Total number of features available per observation:', sum(len(v) for v in data_dict.itervalues())/len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of features available per observation: 21\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Allocation of obervations (POIs vs. Non-POIs)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "for k, v in data_dict.iteritems():\n",
      "    if(data_dict[k][\"poi\"]==1):\n",
      "        count +=1\n",
      "    else:\n",
      "        continue\n",
      "print 'Number of obervations that have POIs:', count\n",
      "print\n",
      "print 'Number of obervations that do not have POIs:', len(data_dict) - count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of obervations that have POIs: 18\n",
        "\n",
        "Number of obervations that do not have POIs: 128\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Salary and Total payments of some high ups @ Enron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'LAY KENNETH L, Salary:', data_dict['LAY KENNETH L']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']\n",
      "print\n",
      "print 'SKILLING JEFFREY K, Salary:', data_dict['SKILLING JEFFREY K']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']\n",
      "print\n",
      "print 'FASTOW ANDREW S: Salary,', data_dict['FASTOW ANDREW S']['salary'],', Total payments:', data_dict['LAY KENNETH L']['total_payments']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LAY KENNETH L, Salary: 1072321 , Total payments: 103559793\n",
        "\n",
        "SKILLING JEFFREY K, Salary: 1111258 , Total payments: 103559793\n",
        "\n",
        "FASTOW ANDREW S: Salary, 440698 , Total payments: 103559793\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Salaried people @ Enron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "salaried_people = 0\n",
      "for k,v in data_dict.iteritems():\n",
      "    if (data_dict[k][\"salary\"]!= 'NaN'):\n",
      "        salaried_people += 1\n",
      "    else:\n",
      "        continue\n",
      "print 'Number of people salaried @ Enron:', salaried_people"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of people salaried @ Enron: 95\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Outlier Investigation (Detection and Removal / Handling)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outliers in the data set can result from some malfunction or data entry errors. I analyzed the financial data document 'enron61702insiderpay.pdf' with naked eye without any code and following are the observations that I think are outliers and should be cleaned away as they do not contain any essential information. I handled the outliers using the piece of code I applied in the lesson 5 excercise in the format: 'dictionary.pop( key, 0 )'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 2: Remove outliers\n",
      "print 'Wendy Grahm only has the feature of directors fees'\n",
      "print\n",
      "### pp.pprint(data_dict['GRAMM WENDY L'])\n",
      "data_dict.pop(\"GRAMM WENDY L\", None)\n",
      "print 'Eugene Lockhart does not have value associated with any feature'\n",
      "print\n",
      "### pp.pprint(data_dict['LOCKHART EUGENE E'])\n",
      "data_dict.pop(\"LOCKHART EUGENE E\", None)\n",
      "print 'Bruce Wrobel only has exercised stock options as feature'\n",
      "print\n",
      "### pp.pprint(data_dict['WROBEL BRUCE'])\n",
      "data_dict.pop(\"WROBEL BRUCE\", None)\n",
      "print 'THE TRAVEL AGENCY IN THE PARK does not seem to be a POI'\n",
      "print\n",
      "### pp.pprint(data_dict['THE TRAVEL AGENCY IN THE PARK'])\n",
      "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", None)\n",
      "print 'TOTAL is not a particular POI as its the total of all financial features'\n",
      "print\n",
      "### pp.pprint(data_dict['TOTAL'])\n",
      "data_dict.pop(\"TOTAL\", None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wendy Grahm only has the feature of directors fees\n",
        "\n",
        "Eugene Lockhart does not have value associated with any feature\n",
        "\n",
        "Bruce Wrobel only has exercised stock options as feature\n",
        "\n",
        "THE TRAVEL AGENCY IN THE PARK does not seem to be a POI\n",
        "\n",
        "TOTAL is not a particular POI as its the total of all financial features\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. What features did you end up using in your POI identifier, and what selection process did you use to pick them?  Did you have to do any scaling?  Why or why not?  As part of the assignment, you should attempt to engineer your own feature that doesn\u2019t come ready-made in the dataset--explain what feature you tried to make, and the rationale behind it.  (You do not necessarily have to use it in the final analysis, only engineer and test it.)  If you used an algorithm like a decision tree, please also give the feature importances of the features that you use."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimized Feature Selection/Engineering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "List of all 21 features are as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pp.pprint(data_dict['LAY KENNETH L'].keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['salary',\n",
        " 'to_messages',\n",
        " 'deferral_payments',\n",
        " 'total_payments',\n",
        " 'exercised_stock_options',\n",
        " 'bonus',\n",
        " 'restricted_stock',\n",
        " 'shared_receipt_with_poi',\n",
        " 'restricted_stock_deferred',\n",
        " 'total_stock_value',\n",
        " 'expenses',\n",
        " 'loan_advances',\n",
        " 'from_messages',\n",
        " 'other',\n",
        " 'from_this_person_to_poi',\n",
        " 'poi',\n",
        " 'director_fees',\n",
        " 'deferred_income',\n",
        " 'long_term_incentive',\n",
        " 'email_address',\n",
        " 'from_poi_to_this_person']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Initial Feature Selection (by hand from the original features in the datset)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 1: Select what features you'll use.\n",
      "### features_list is a list of strings, each of which is a feature name.\n",
      "### The first feature must be \"poi\".\n",
      "\n",
      "poi = 'poi'\n",
      "\n",
      "e_feature_1 = 'from_messages'\n",
      "e_feature_2 = 'to_messages'\n",
      "e_feature_3 = 'shared_receipt_with_poi'\n",
      "\n",
      "\n",
      "e_features_list = [e_feature_1, e_feature_2, e_feature_3]\n",
      "\n",
      "f_feature_1 = 'salary'\n",
      "f_feature_2 = 'loan_advances'\n",
      "f_feature_3 = 'bonus'\n",
      "f_feature_4 = 'total_stock_value'\n",
      "f_feature_5 = 'exercised_stock_options'\n",
      "\n",
      "f_features_list = [f_feature_1, f_feature_2, f_feature_3, f_feature_4, f_feature_5]\n",
      "\n",
      "#fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "features_list = [poi] + e_features_list + f_features_list\n",
      "\n",
      "print 'Number of features used from the original data set:', len(features_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features used from the original data set: 9\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Invesitgation/Relationship of some of the features that were selected by hand"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_list = [poi] + e_features_list + f_features_list\n",
      "\n",
      "data = featureFormat(my_dataset, features_list )\n",
      "poi, testing_poi_test_features = targetFeatureSplit(data)\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][1], testing_poi_test_features[ii][2], color = 'c')\n",
      "    plt.xlabel('to_messages')\n",
      "    plt.ylabel('shared_receipt_with_poi')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][1], testing_poi_test_features[ii][2], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][0], testing_poi_test_features[ii][2], color = 'c')\n",
      "    plt.xlabel('from_messages')\n",
      "    plt.ylabel('shared_receipt_with_poi')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][0], testing_poi_test_features[ii][2], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][3], testing_poi_test_features[ii][5], color = 'c')\n",
      "    plt.xlabel('salary')\n",
      "    plt.ylabel('bonus')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][3], testing_poi_test_features[ii][5], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n",
      "\n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    plt.scatter(testing_poi_test_features[ii][6], testing_poi_test_features[ii][5], color = 'c')\n",
      "    plt.xlabel('exercised_stock_options')\n",
      "    plt.ylabel('bonus')\n",
      "   \n",
      "for ii, pp in enumerate(testing_poi_test_features):\n",
      "    if poi[ii]:\n",
      "        plt.scatter(testing_poi_test_features[ii][6], testing_poi_test_features[ii][5], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'my_dataset' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-23cdf30edefb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpoi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0me_features_list\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf_features_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpoi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_poi_test_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'my_dataset' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "New Feature Creation - 1. Fraction to/from POI emails (2 features) - Combination of original features in the dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the exercise quiz in lesson 11, following code was implemented to include 2 new features in the data set namely 'fraction_from_poi' and 'fraction_to_poi'. These features are a combination of 4 original features in the dataset. After creation, features are then stored in the modified feature list, 'my_feature_list' and values for all observations stored in modified data set, 'my_dataset'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Task 3: Create new feature(s)\n",
      "my_feature_list = []\n",
      "my_dataset = {}\n",
      "\n",
      "def computeFraction(poi_messages, all_messages):\n",
      "    \"\"\" given a number messages to/from POI (numerator) \n",
      "        and number of all messages to/from a person (denominator),\n",
      "        return the fraction of messages to/from that person\n",
      "        that are from/to a POI\n",
      "   \"\"\"\n",
      "\n",
      "\n",
      "    ### you fill in this code, so that it returns either\n",
      "    ###     the fraction of all messages to this person that come from POIs\n",
      "    ###     or\n",
      "    ###     the fraction of all messages from this person that are sent to POIs\n",
      "    ### the same code can be used to compute either quantity\n",
      "\n",
      "    ### beware of \"NaN\" when there is no known email address (and so\n",
      "    ### no filled email features), and integer division!\n",
      "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
      "    if (poi_messages == 'NaN') or (all_messages == 'NaN'):\n",
      "        fraction = 0.\n",
      "    else:\n",
      "        fraction = float(poi_messages)/float(all_messages)\n",
      "\n",
      "    return fraction\n",
      "\n",
      "my_dataset = data_dict\n",
      "\n",
      "\n",
      "for name in data_dict:\n",
      "    #print name\n",
      "    data_point = data_dict[name]\n",
      "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
      "    to_messages = data_point[\"to_messages\"]\n",
      "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
      "    #print fraction_from_poi\n",
      "    \n",
      "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
      "    from_messages = data_point[\"from_messages\"]\n",
      "    fraction_to_poi = computeFraction(from_this_person_to_poi, from_messages)\n",
      "    #print fraction_to_poi\n",
      "    \n",
      "    ### Adding values of new features to the modified dataset, 'my_dataset'\n",
      "    \n",
      "    my_dataset[name]['fraction_from_poi'] = fraction_from_poi\n",
      "    my_dataset[name]['fraction_to_poi'] = fraction_to_poi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Adding new feature to the modified features list, 'my_feature_list'\n",
      "\n",
      "fraction_features = ['fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "my_feature_list = features_list + fraction_features\n",
      "\n",
      "print 'Number of features now:', len(my_feature_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features now: 11\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poi  = \"poi\"\n",
      "\n",
      "poi_fraction_features = [poi, 'fraction_from_poi', 'fraction_to_poi']\n",
      "\n",
      "data = featureFormat(my_dataset, poi_fraction_features)\n",
      "poi, testing_fraction_features = targetFeatureSplit(data)\n",
      "\n",
      "for ii, pp in enumerate(testing_fraction_features):\n",
      "    if testing_fraction_features[ii][0] != 0.0 or testing_fraction_features[ii][1] != 0.0:\n",
      "        plt.scatter(testing_fraction_features[ii][0], testing_fraction_features[ii][1], color = 'c')\n",
      "        plt.xlabel('Fraction of emails this person gets from POI')\n",
      "        plt.ylabel('Fraction of emails this person sends to POI')\n",
      "    else:\n",
      "        continue\n",
      "for ii, pp in enumerate(testing_fraction_features):\n",
      "    if poi[ii] and (testing_fraction_features[ii][0] != 0.0 or testing_fraction_features[ii][1] != 0.0):\n",
      "        plt.scatter(testing_fraction_features[ii][0], testing_fraction_features[ii][1], color = 'r', marker=\"+\")   \n",
      "        \n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3lS7ZIKdAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x180a80b8>"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NOTE: The scatter plot represents the fraction of email exchange between Non-POI/POI. Red crosses show the POIs. It can be observed that there is a definite trend that POIs mostly send/receive emails to/from other POIs. People that are non POI tend to send fewer emails as can be seen mostly clustered with fractions closer to 0."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "New Feature Creation - 2. Features from text in the email archives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pasreOutText function is used from the lesson 10 mini-project along with a modified version of the code used to extract email text in lesson 10 mini-project. Firstm it was checked if the observation had valid email address and then for a valid email address, email text was extracted using the pasreOutText function. The snowball stemmer for english is already implemented in the pasreOutText function. The text features data and the user labels data are extracted and stored in 'your_word_data.pkl' and 'your_email_authors.pkl' files respectively. Using words in emails as feature will add an additional dimension to he model and might be useful in creating the POI identifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseOutText(f):\n",
      "    \"\"\" given an opened email file f, parse out all text below the\n",
      "        metadata block at the top\n",
      "        (in Part 2, you will also add stemming capabilities)\n",
      "        and return a string that contains all the words\n",
      "        in the email (space-separated) \n",
      "        \n",
      "        example use case:\n",
      "        f = open(\"email_file_name.txt\", \"r\")\n",
      "        text = parseOutText(f)\n",
      "        \n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "    f.seek(0)  ### go back to beginning of file (annoying)\n",
      "    all_text = f.read()\n",
      "\n",
      "    ### split off metadata\n",
      "    content = all_text.split(\"X-FileName:\")\n",
      "    words = \"\"\n",
      "    if len(content) > 1:\n",
      "        ### remove punctuation\n",
      "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
      "\n",
      "        ### project part 2: comment out the line below\n",
      "        #words = text_string\n",
      "\n",
      "        ### split the text string into individual words, stem each word,\n",
      "        ### and append the stemmed word to words (make sure there's a single\n",
      "        ### space between each stemmed word)\n",
      "        \n",
      "        #print text_string\n",
      "        \n",
      "        email_split = text_string.split()\n",
      "    \n",
      "        #print email_split\n",
      "\n",
      "        from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "        stemmer = SnowballStemmer(\"english\")\n",
      "        \n",
      "        email_stemmed = []\n",
      "        for n in range(len(email_split)):\n",
      "            email_stemmed.append(stemmer.stem(email_split[n]))\n",
      "        \n",
      "    words = ' '.join(email_stemmed)    \n",
      "    \n",
      "    return words\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Piece of code to identify valid email addresses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "valid_email_id_list = []\n",
      "\n",
      "email_address_path_folder = os.listdir(\"../final_project/emails_by_address/\")\n",
      "\n",
      "test_dataset = my_dataset\n",
      "#observation_removal = []\n",
      "\n",
      "count1 = 0\n",
      "count2 = 0\n",
      "\n",
      "for name in my_dataset:\n",
      "    #k = name\n",
      "    email_name = test_dataset[name]\n",
      "    email_address = email_name[\"email_address\"]\n",
      "    email_address_path = \"from_\" + email_address + \".txt\"\n",
      "    #print email_address, email_address_path\n",
      "    if email_address == 'NaN':\n",
      "        #observation_removal.append(name)\n",
      "        #print 'TO BE REMOVED_1:' ,email_address_path\n",
      "        count1 += 1\n",
      "    \n",
      "    elif email_address_path not in email_address_path_folder:\n",
      "        #observation_removal.append(name)\n",
      "        #print 'TO BE REMOVED_2:' ,email_address_path\n",
      "        count2 += 1\n",
      "    else:\n",
      "        valid_email_id_list.append(email_address)\n",
      "        #print email_address_path\n",
      "    \n",
      "\n",
      "print 'size of the dataset:', len(my_dataset)    \n",
      "print    \n",
      "print 'No of observation that have no email data for word features to be created:', count1 + count2 \n",
      "\n",
      "\n",
      "'''        \n",
      "for n in observation_removal:\n",
      "    test_dataset.pop(n, None)\n",
      "'''            \n",
      "\n",
      "my_dataset = test_dataset       \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of the dataset: 141\n",
        "\n",
        "No of observation that have no email data for word features to be created: 55\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print valid_email_id_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['mark.metts@enron.com', 'bill.cordes@enron.com', 'kevin.hannon@enron.com', 'rockford.meyer@enron.com', 'jeffrey.mcmahon@enron.com', 'stanley.horton@enron.com', 'greg.piper@enron.com', 'gene.humphrey@enron.com', 'adam.umanoff@enron.com', 'jeremy.blachman@enron.com', 'marty.sunde@enron.com', 'dana.gibbs@enron.com', 'wes.colwell@enron.com', 's..muller@enron.com', 'charlene.jackson@enron.com', 'rob.walls@enron.com', 'louise.kitchen@enron.com', 'jeffrey.shankman@enron.com', 'rick.bergsieker@enron.com', 'philippe.bibi@enron.com', 'paula.rieker@enron.com', 'sally.beck@enron.com', 'david.haug@enron.com', 'gary.hickerson@enron.com', 'richard.lewis@enron.com', 'robert.hayes@enron.com', 'danny.mccarty@enron.com', 'dan.leff@enron.com', 'john.lavorato@enron.com', 'ken.powers@enron.com', 'james.bannantine@enron.com', 'richard.shapiro@enron.com', 'john.sherriff@enron.com', 'rex.shelby@enron.com', 'joseph.deffner@enron.com', 'greg.whalley@enron.com', 'mike.mcconnell@enron.com', 'jim.piro@enron.com', 'david.delainey@enron.com', 'kenneth.lay@enron.com', 'cindy.olson@enron.com', 'rebecca.mcdonald@enron.com', 'george.mcclellan@enron.com', 'mark.haedicke@enron.com', 'raymond.bowen@enron.com', 'jay.fitzgerald@enron.com', 'michael.moran@enron.com', 'brian.redmond@enron.com', 'tim.belden@enron.com', 'w.duran@enron.com', 'terence.thorn@enron.com', 'tracy.foy@enron.com', 'christopher.calger@enron.com', 'ken.rice@enron.com', 'vince.kaminski@enron.com', 'chip.cox@enron.com', 'jeff.skilling@enron.com', 'jeffrey.sherrick@enron.com', 'mark.pickering@enron.com', 'steven.kean@enron.com', 'kulvinder.fowler@enron.com', 'george.wasaff@enron.com', 'phillip.allen@enron.com', 'vicki.sharp@enron.com', 'michael.brown@enron.com', 'james.hughes@enron.com', 'sanjay.bhatnagar@enron.com', 'rebecca.carter@enron.com', 'john.buchanan@enron.com', 'julia.murray@enron.com', 'kevin.garland@enron.com', 'keith.dodson@enron.com', 'janet.dietrich@enron.com', 'james.derrick@enron.com', 'mark.frevert@enron.com', 'rod.hayslett@enron.com', 'jim.fallon@enron.com', 'mark.koenig@enron.com', 'larry.izzo@enron.com', 'elizabeth.tilney@enron.com', 'a..martin@enron.com', 'rick.buy@enron.com', 'richard.causey@enron.com', 'mitchell.taylor@enron.com', 'jeff.donahue@enron.com', 'ben.glisan@enron.com']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Final piece of code used to parse email text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from_data = []\n",
      "word_data = []\n",
      "\n",
      "import os\n",
      "import pickle\n",
      "import re\n",
      "import sys\n",
      "import string\n",
      "\n",
      "\n",
      "temp_counter = 0\n",
      "\n",
      "for n in range(len(valid_email_id_list)):\n",
      "    \n",
      "    email_address_folder = \"../final_project/emails_by_address/from_\" + valid_email_id_list[n] + \".txt\"\n",
      "    \n",
      "    #text_file = os.listdir(\"../final_project/emails_by_address/\")\n",
      "    \n",
      "    email_file_path = open(email_address_folder, \"r\")\n",
      "    \n",
      "    for path in email_file_path:\n",
      "        ### only look at first 200 emails when developing\n",
      "        ### once everything is working, remove this line to run over full dataset\n",
      "        temp_counter += 1\n",
      "        if temp_counter < 500:\n",
      "        \n",
      "        \n",
      "            path = os.path.join('..', path[:-1])\n",
      "            #print path\n",
      "            email = open(path, \"r\")\n",
      "\n",
      "            ### use parseOutText to extract the text from the opened email\n",
      "\n",
      "            ### use str.replace() to remove any instances of the words\n",
      "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
      "\n",
      "            ### append the text to word_data\n",
      "\n",
      "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
      "\n",
      "            stemmed_email = parseOutText(email)\n",
      "\n",
      "            word_data.append(stemmed_email)\n",
      "            '''\n",
      "            if (name == \"chris\"):\n",
      "                from_data.append(1)\n",
      "            else:\n",
      "                from_data.append(0)\n",
      "            '''\n",
      "            from_data.append(n)\n",
      "\n",
      "            email.close()\n",
      "    \n",
      "print \"emails processed\"\n",
      "#from_sara.close()\n",
      "#from_chris.close()\n",
      "\n",
      "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
      "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "emails processed\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = featureFormat(my_dataset, features_list)\n",
      "\n",
      "labels, features = targetFeatureSplit(data)\n",
      "\n",
      "#print labels\n",
      "\n",
      "#print features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print from_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn.feature_selection import SelectPercentile, f_classif\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "\n",
      "skf = StratifiedKFold(labels, n_folds=3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print skf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=3)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "\n",
      "    word_features_train = vectorizer.fit_transform(\n",
      "        word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "\n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 3\n",
      "print \"Average precision: \", sum(precision) / 3\n",
      "print \"Average recall: \", sum(recall) / 3\n",
      "print \"Average f1_score: \", sum(f1) / 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  0.840909090909\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.727272727273\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.837209302326\n",
        "Precision:  0.333333333333\n",
        "Recall:  0.166666666667\n",
        "F1_Score:  0.222222222222\n",
        "Average accuracy:  0.801797040169\n",
        "Average precision:  0.111111111111\n",
        "Average recall:  0.0555555555556\n",
        "Average f1_score:  0.0740740740741\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=3)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "\n",
      "    word_features_train = vectorizer.fit_transform(\n",
      "        word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "\n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 3\n",
      "print \"Average precision: \", sum(precision) / 3\n",
      "print \"Average recall: \", sum(recall) / 3\n",
      "print \"Average f1_score: \", sum(f1) / 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  0.840909090909\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.727272727273\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.837209302326\n",
        "Precision:  0.333333333333\n",
        "Recall:  0.166666666667\n",
        "F1_Score:  0.222222222222\n",
        "Average accuracy:  0.801797040169\n",
        "Average precision:  0.111111111111\n",
        "Average recall:  0.0555555555556\n",
        "Average f1_score:  0.0740740740741\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=4)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "\n",
      "    word_features_train = vectorizer.fit_transform(\n",
      "        word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "\n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "    '''\n",
      "    class sklearn.grid_search.RandomizedSearchCV(estimator, param_distributions,\n",
      "                                                 n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True,\n",
      "                                                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                                                 error_score='raise')[source]\n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = RandomizedSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 4\n",
      "print \"Average precision: \", sum(precision) / 4\n",
      "print \"Average recall: \", sum(recall) / 4\n",
      "print \"Average f1_score: \", sum(f1) / 4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  0.840909090909\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.727272727273\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.837209302326\n",
        "Precision:  0.333333333333\n",
        "Recall:  0.166666666667\n",
        "F1_Score:  0.222222222222\n",
        "Average accuracy:  0.601347780127\n",
        "Average precision:  0.0833333333333\n",
        "Average recall:  0.0416666666667\n",
        "Average f1_score:  0.0555555555556\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=6)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "\n",
      "    word_features_train = vectorizer.fit_transform(word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "\n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "    '''\n",
      "    class sklearn.grid_search.RandomizedSearchCV(estimator, param_distributions,\n",
      "                                                 n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True,\n",
      "                                                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                                                 error_score='raise')[source]\n",
      "    '''\n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 6\n",
      "print \"Average precision: \", sum(precision) / 6\n",
      "print \"Average recall: \", sum(recall) / 6\n",
      "print \"Average f1_score: \", sum(f1) / 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy:  0.863636363636\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.818181818182\n",
        "Precision:  0.0\n",
        "Recall:  0.0\n",
        "F1_Score:  0.0\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.681818181818\n",
        "Precision:  0.25\n",
        "Recall:  0.666666666667\n",
        "F1_Score:  0.363636363636\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.909090909091\n",
        "Precision:  1.0\n",
        "Recall:  0.333333333333\n",
        "F1_Score:  0.5\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.909090909091\n",
        "Precision:  1.0\n",
        "Recall:  0.333333333333\n",
        "F1_Score:  0.5\n",
        "Accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.714285714286\n",
        "Precision:  0.2\n",
        "Recall:  0.333333333333\n",
        "F1_Score:  0.25\n",
        "Average accuracy:  0.816017316017\n",
        "Average precision:  0.408333333333\n",
        "Average recall:  0.277777777778\n",
        "Average f1_score:  0.268939393939\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "accuracy = []\n",
      "precision = []\n",
      "recall = []\n",
      "f1 = []\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True, sublinear_tf=True, max_df=0.5)\n",
      "scaler = MinMaxScaler()\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "skf = StratifiedKFold(labels, n_folds=10)\n",
      "\n",
      "for train_idx, test_idx in skf:\n",
      "    features_train = []\n",
      "    features_test = []\n",
      "    word_features_train = []\n",
      "    word_features_test = []\n",
      "    labels_train = []\n",
      "    labels_test = []\n",
      "\n",
      "    for ii in train_idx:\n",
      "        features_train.append(features[ii])\n",
      "        word_features_train.append(word_data[ii])\n",
      "        labels_train.append(labels[ii])\n",
      "    for jj in test_idx:\n",
      "        features_test.append(features[jj])\n",
      "        word_features_test.append(word_data[jj])\n",
      "        labels_test.append(labels[jj])\n",
      "\n",
      "    word_features_train = vectorizer.fit_transform(\n",
      "        word_features_train).toarray()\n",
      "    features_train = np.hstack((features_train, word_features_train))\n",
      "    word_features_test = vectorizer.transform(word_features_test).toarray()\n",
      "    features_test = np.hstack((features_test, word_features_test))\n",
      "\n",
      "    features_train = scaler.fit_transform(features_train)\n",
      "    features_train = selector.fit_transform(features_train, labels_train)\n",
      "    features_test = scaler.transform(features_test)\n",
      "    features_test = selector.transform(features_test)\n",
      "    '''\n",
      "    class sklearn.grid_search.RandomizedSearchCV(estimator, param_distributions,\n",
      "                                                 n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True,\n",
      "                                                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                                                 error_score='raise')[source]\n",
      "    \n",
      "    ada = AdaBoostClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(ada, parameters)\n",
      "    clf.fit(features_train, labels_train)\n",
      "    pred = clf.predict(features_test)\n",
      "    '''\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "    #rfc = RandomForestClassifier()  \n",
      "    #parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    #clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5)\n",
      "\n",
      "    \n",
      "    from sklearn import tree\n",
      "    \n",
      "    dtc = tree.DecisionTreeClassifier()\n",
      "    parameters = {'n_estimators': [1, 100], 'random_state': [1, 50]}\n",
      "    clf = GridSearchCV(dtc, parameters)\n",
      "    #print 'hello again'\n",
      "    clf.fit(features_train, labels_train)\n",
      "    #print 'hello hello'\n",
      "\n",
      "    pred = clf.predict(features_test)\n",
      "\n",
      "    \n",
      "    print \"Accuracy: \", clf.score(features_test, labels_test)\n",
      "    print \"Precision: \", precision_score(labels_test, pred)\n",
      "    print \"Recall: \", recall_score(labels_test, pred)\n",
      "    print \"F1_Score: \", f1_score(labels_test, pred)\n",
      "\n",
      "    accuracy.append(clf.score(features_test, labels_test))\n",
      "    precision.append(precision_score(labels_test, pred))\n",
      "    recall.append(recall_score(labels_test, pred))\n",
      "    f1.append(f1_score(labels_test, pred))\n",
      "\n",
      "print \"Average accuracy: \", sum(accuracy) / 10\n",
      "print \"Average precision: \", sum(precision) / 10\n",
      "print \"Average recall: \", sum(recall) / 10\n",
      "print \"Average f1_score: \", sum(f1) / 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Invalid parameter n_estimators for estimator DecisionTreeClassifier",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-55-b6d8f12117e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m#print 'hello again'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;31m#print 'hello hello'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    376\u001b[0m                                     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m                                     self.fit_params, return_parameters=True)\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             for train, test in cv)\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \"\"\"\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sk988f\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    254\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                     raise ValueError('Invalid parameter %s ' 'for estimator %s'\n\u001b[1;32m--> 256\u001b[1;33m                                      % (key, self.__class__.__name__))\n\u001b[0m\u001b[0;32m    257\u001b[0m                 \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Invalid parameter n_estimators for estimator DecisionTreeClassifier"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}